{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make vocab file for transformers, and add special tokens in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import tqdm\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "from PIL import Image\n",
    "import argparse\n",
    "\n",
    "from catr.models import caption\n",
    "from catr.models import utils as mtils\n",
    "from catr.datasets import coco, utils\n",
    "from catr.cfg_damsm_vocab import Config\n",
    "\n",
    "import json, pickle\n",
    "from pycocotools.coco import COCO as CC\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27297"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## try bert tokenizer used in CATR\n",
    "fpath = '/media/MyDataStor1/mmrl/MMRL/data/coco/captions.pickle'\n",
    "with open(fpath, 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "train_captions, test_captions, ixtoword, wordtoix = vocab\n",
    "len(ixtoword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ixtoword[27297] = '[CLS]'\n",
    "ixtoword[27298] = '[SEP]'\n",
    "ixtoword[27299] = '[UNK]'\n",
    "ixtoword[0] = '[PAD]'\n",
    "wordtoix['[CLS]'] = 27297\n",
    "wordtoix['[SEP]'] = 27298\n",
    "wordtoix['[UNK]'] = 27299\n",
    "wordtoix['[PAD]'] = 0\n",
    "wordtoix.pop('<end>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27300 27300\n",
      "27297 27298 27299 0\n",
      "[CLS] [SEP] [UNK] [PAD]\n"
     ]
    }
   ],
   "source": [
    "print(len(ixtoword),len(wordtoix))\n",
    "print(wordtoix['[CLS]'], wordtoix['[SEP]'], wordtoix['[UNK]'], wordtoix['[PAD]'])\n",
    "print(ixtoword[27297], ixtoword[27298], ixtoword[27299], ixtoword[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save ixtoword to .txt file\n",
    "vocab_txt = 'catr/damsm_vocab.txt'\n",
    "with open(vocab_txt, 'w') as f:\n",
    "    for i in range(27300):\n",
    "        f.write(\"%s\\n\" % ixtoword[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27300\n",
      "27297\n"
     ]
    }
   ],
   "source": [
    "## test bert_tokenizer on damsm new vocab_dict\n",
    "damsm_tker = BertTokenizer.from_pretrained(\"catr/damsm_vocab.txt\", do_lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27300\n",
      "[CLS] 27297\n",
      "[SEP] 27298\n",
      "[PAD] 0\n",
      "[UNK] 27299\n"
     ]
    }
   ],
   "source": [
    "print(damsm_tker.vocab_size)\n",
    "print(damsm_tker._cls_token, damsm_tker.convert_tokens_to_ids(damsm_tker._cls_token))\n",
    "print(damsm_tker._sep_token, damsm_tker.convert_tokens_to_ids(damsm_tker._sep_token))\n",
    "print(damsm_tker._pad_token, damsm_tker.convert_tokens_to_ids(damsm_tker._pad_token))\n",
    "print(damsm_tker._unk_token, damsm_tker.convert_tokens_to_ids(damsm_tker._unk_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[27297, 10, 80, 115, 137, 154, 27298, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a man is playing ball']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['a man is playing ball']\n",
    "tks = damsm_tker(sentences, padding='max_length', return_attention_mask=True, \n",
    "                 return_token_type_ids=False, truncation=True, max_length=129)\n",
    "print(tks)\n",
    "damsm_tker.batch_decode(tks['input_ids'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "# tokenizer used in damsm dataloader\n",
    "sentences = ['A dirt path with a young person on a motor bike rests to the foreground of a verdant area with a bridge and a background of cloud-wreathed mountains']\n",
    "reg_tk = RegexpTokenizer(r'\\w+')\n",
    "regds = reg_tk.tokenize(sentences[0].lower())\n",
    "print(len(regds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the raw tokens from Show, Attention, Tell paper\n",
    "with open('catr/coco/dataset.json', 'r') as f:\n",
    "    otk = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filepath': 'val2014',\n",
       " 'sentids': [770337, 771687, 772707, 776154, 781998],\n",
       " 'filename': 'COCO_val2014_000000391895.jpg',\n",
       " 'imgid': 0,\n",
       " 'split': 'test',\n",
       " 'sentences': [{'tokens': ['a',\n",
       "    'man',\n",
       "    'with',\n",
       "    'a',\n",
       "    'red',\n",
       "    'helmet',\n",
       "    'on',\n",
       "    'a',\n",
       "    'small',\n",
       "    'moped',\n",
       "    'on',\n",
       "    'a',\n",
       "    'dirt',\n",
       "    'road'],\n",
       "   'raw': 'A man with a red helmet on a small moped on a dirt road. ',\n",
       "   'imgid': 0,\n",
       "   'sentid': 770337},\n",
       "  {'tokens': ['man',\n",
       "    'riding',\n",
       "    'a',\n",
       "    'motor',\n",
       "    'bike',\n",
       "    'on',\n",
       "    'a',\n",
       "    'dirt',\n",
       "    'road',\n",
       "    'on',\n",
       "    'the',\n",
       "    'countryside'],\n",
       "   'raw': 'Man riding a motor bike on a dirt road on the countryside.',\n",
       "   'imgid': 0,\n",
       "   'sentid': 771687},\n",
       "  {'tokens': ['a',\n",
       "    'man',\n",
       "    'riding',\n",
       "    'on',\n",
       "    'the',\n",
       "    'back',\n",
       "    'of',\n",
       "    'a',\n",
       "    'motorcycle'],\n",
       "   'raw': 'A man riding on the back of a motorcycle.',\n",
       "   'imgid': 0,\n",
       "   'sentid': 772707},\n",
       "  {'tokens': ['a',\n",
       "    'dirt',\n",
       "    'path',\n",
       "    'with',\n",
       "    'a',\n",
       "    'young',\n",
       "    'person',\n",
       "    'on',\n",
       "    'a',\n",
       "    'motor',\n",
       "    'bike',\n",
       "    'rests',\n",
       "    'to',\n",
       "    'the',\n",
       "    'foreground',\n",
       "    'of',\n",
       "    'a',\n",
       "    'verdant',\n",
       "    'area',\n",
       "    'with',\n",
       "    'a',\n",
       "    'bridge',\n",
       "    'and',\n",
       "    'a',\n",
       "    'background',\n",
       "    'of',\n",
       "    'cloud',\n",
       "    'wreathed',\n",
       "    'mountains'],\n",
       "   'raw': 'A dirt path with a young person on a motor bike rests to the foreground of a verdant area with a bridge and a background of cloud-wreathed mountains. ',\n",
       "   'imgid': 0,\n",
       "   'sentid': 776154},\n",
       "  {'tokens': ['a',\n",
       "    'man',\n",
       "    'in',\n",
       "    'a',\n",
       "    'red',\n",
       "    'shirt',\n",
       "    'and',\n",
       "    'a',\n",
       "    'red',\n",
       "    'hat',\n",
       "    'is',\n",
       "    'on',\n",
       "    'a',\n",
       "    'motorcycle',\n",
       "    'on',\n",
       "    'a',\n",
       "    'hill',\n",
       "    'side'],\n",
       "   'raw': 'A man in a red shirt and a red hat is on a motorcycle on a hill side.',\n",
       "   'imgid': 0,\n",
       "   'sentid': 781998}],\n",
       " 'cocoid': 391895}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "otk['images'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "{'input_ids': [[101, 1037, 9033, 12036, 2099, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a sizzor']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentences = ['This framework generates embeddings for each input sentence',\n",
    "#              'Sentences are passed as a list of string.',\n",
    "#              'The quick brown fox jumps over the lazy dog.']\n",
    "sentences = ['a sizzor']\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower=True)\n",
    "print(bert_tokenizer.vocab_size)\n",
    "bkds = bert_tokenizer(sentences, padding='max_length', return_attention_mask=True, \n",
    "                 return_token_type_ids=False, truncation=True, max_length=129)\n",
    "print(bkds)\n",
    "\n",
    "bert_tokenizer.batch_decode(bkds['input_ids'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2023, 7705, 19421, 7861, 8270, 4667, 2015, 2005, 2169, 7953, 6251, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 11746, 2024, 2979, 2004, 1037, 2862, 1997, 5164, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## another tokenizer used in AttnGAN by Abuzar\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "             'Sentences are passed as a list of string.',\n",
    "             'The quick brown fox jumps over the lazy dog.']\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/bert-base-nli-mean-tokens\")\n",
    "print(tokenizer.vocab_size)\n",
    "tkds = tokenizer(sentences, padding='max_length', return_attention_mask=True, \n",
    "                 return_token_type_ids=False, truncation=True, max_length=129)\n",
    "tkds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test the commas in the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"catr/damsm_vocab.txt\", do_lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a dog is playing on the whole-green grass's.   \n",
      "a dog is playing on the whole green grass s   \n",
      "[27297, 10, 108, 115, 137, 11, 8, 1885, 41, 200, 84, 27298, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "sss = 'a dog is playing on the whole-green grass\\'s.   '\n",
    "print(sss)\n",
    "sss = sss.replace('.', '').replace('\\'', ' ').replace('-', ' ')\n",
    "print(sss)\n",
    "ttt = tokenizer.encode_plus(\n",
    "            sss, max_length=129, padding='max_length', return_attention_mask=True, return_token_type_ids=False, truncation=True)\n",
    "print(ttt['input_ids'][:20], ttt['attention_mask'][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the captions and caption_masks in training data and val data to save time and get rid of special marks in the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414113\n",
      "{'image_id': 133071, 'id': 829717, 'caption': 'A dinner plate has a lemon wedge garnishment.'}\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(\n",
    "            '/media/MyDataStor1/mmrl/MMRL/data/coco', 'annotations', 'captions_train2014.json'), 'rb') as f:\n",
    "    test = json.load(f)\n",
    "print(len(test['annotations']))\n",
    "print(test['annotations'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90149\n",
      "122899\n",
      "155889\n",
      "221020\n",
      "256632\n",
      "286858\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(d['annotations'])):\n",
    "    if 'mmm' in d['annotations'][i]['caption']:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414113\n",
      "dict_keys(['info', 'images', 'licenses', 'annotations'])\n",
      "[{'image_id': 318556, 'id': 48, 'caption': 'A very clean and well decorated empty bathroom'}, {'image_id': 116100, 'id': 67, 'caption': 'A panoramic view of a kitchen and all of its appliances.'}]\n"
     ]
    }
   ],
   "source": [
    "## training set\n",
    "train_file = os.path.join(\n",
    "            '/media/MyDataStor1/mmrl/MMRL/data/coco', 'annotations', 'captions_train2014.json')\n",
    "with open(train_file, 'r') as f:\n",
    "    d = json.load(f)\n",
    "print(len(d['annotations']))\n",
    "print(d.keys())\n",
    "print(d['annotations'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"catr/damsm_vocab.txt\", do_lower=True)\n",
    "reger = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = 'a dog is playing on the whole-green grass\\'s. '\n",
    "for i in range(len(d['annotations'])):\n",
    "    sentence = d['annotations'][i]['caption']\n",
    "    regtk = reger.tokenize(sentence.lower())\n",
    "    reg_cap = ' '.join(regtk)\n",
    "    caption_encoded = tokenizer.encode_plus(\n",
    "                reg_cap, max_length=129, padding='max_length', return_attention_mask=True, return_token_type_ids=False, truncation=True)\n",
    "    d['annotations'][i]['caption'] = caption_encoded['input_ids']\n",
    "    d['annotations'][i]['cap_mask'] = caption_encoded['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414113\n",
      "dict_keys(['image_id', 'id', 'caption', 'cap_mask'])\n",
      "129 129\n",
      "{'image_id': 116100, 'id': 67, 'caption': [27297, 10, 6766, 415, 21, 10, 288, 58, 48, 21, 343, 1101, 27298, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'cap_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a panoramic view of a kitchen and all of its appliances'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 1\n",
    "print(len(d['annotations']))\n",
    "print(d['annotations'][i].keys())\n",
    "print(len(d['annotations'][i]['caption']), len(d['annotations'][i]['cap_mask']))\n",
    "print(d['annotations'][i])\n",
    "\n",
    "tokenizer.decode(d['annotations'][i]['caption'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_pk_file = os.path.join(\n",
    "            '/media/MyDataStor1/mmrl/MMRL/data/coco', 'annotations', 'tokens_train2014.json')\n",
    "with open(tr_pk_file, 'w') as f:\n",
    "#     pickle.dump(d, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    json.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202654\n",
      "dict_keys(['info', 'images', 'licenses', 'annotations'])\n",
      "[{'image_id': 203564, 'id': 37, 'caption': 'A bicycle replica with a clock as the front wheel.'}, {'image_id': 179765, 'id': 38, 'caption': 'A black Honda motorcycle parked in front of a garage.'}]\n"
     ]
    }
   ],
   "source": [
    "val_file = os.path.join(\n",
    "            '/media/MyDataStor1/mmrl/MMRL/data/coco', 'annotations', 'captions_val2014.json')\n",
    "with open(val_file, 'r') as f:\n",
    "    g = json.load(f)\n",
    "print(len(g['annotations']))\n",
    "print(g.keys())\n",
    "print(g['annotations'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"catr/damsm_vocab.txt\", do_lower=True)\n",
    "reger = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = 'a dog is playing on the whole-green grass\\'s. '\n",
    "for i in range(len(g['annotations'])):\n",
    "    sentence = g['annotations'][i]['caption']\n",
    "    regtk = reger.tokenize(sentence.lower())\n",
    "    reg_cap = ' '.join(regtk)\n",
    "    caption_encoded = tokenizer.encode_plus(\n",
    "                reg_cap, max_length=129, padding='max_length', return_attention_mask=True, return_token_type_ids=False, truncation=True)\n",
    "    g['annotations'][i]['caption'] = caption_encoded['input_ids']\n",
    "    g['annotations'][i]['cap_mask'] = caption_encoded['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202654\n",
      "dict_keys(['image_id', 'id', 'caption', 'cap_mask'])\n",
      "129 129\n",
      "{'image_id': 179765, 'id': 38, 'caption': [27297, 10, 518, 3812, 206, 36, 7, 54, 21, 10, 4894, 27298, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'cap_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a black honda motorcycle parked in front of a garage'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 1\n",
    "print(len(g['annotations']))\n",
    "print(g['annotations'][i].keys())\n",
    "print(len(g['annotations'][i]['caption']), len(g['annotations'][i]['cap_mask']))\n",
    "print(g['annotations'][i])\n",
    "\n",
    "tokenizer.decode(g['annotations'][i]['caption'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vl_pk_file = os.path.join(\n",
    "            '/media/MyDataStor1/mmrl/MMRL/data/coco', 'annotations', 'tokens_val2014.json')\n",
    "with open(vl_pk_file, 'w') as f:\n",
    "#     pickle.dump(g, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    json.dump(g, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
