{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from miscc.utils import mkdir_p\n",
    "from miscc.utils import build_super_images\n",
    "from miscc.losses import sent_loss, words_loss\n",
    "from miscc.config import cfg, cfg_from_file\n",
    "\n",
    "from datasets import TextDataset\n",
    "from datasets import prepare_data\n",
    "\n",
    "from model import RNN_ENCODER, CNN_ENCODER\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import pprint\n",
    "import datetime\n",
    "import dateutil.tz\n",
    "import argparse\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# dir_path = (os.path.abspath(os.path.join(os.path.realpath(__file__), './.')))\n",
    "# sys.path.append(dir_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class parse_args():\n",
    "    cfg_file='../code/cfg/DAMSM/bird.yml'\n",
    "    gpu_id=1\n",
    "    data_dir='../data/birds/'\n",
    "    manualSeed=1\n",
    "args = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using config:\n",
      "{'B_VALIDATION': False,\n",
      " 'CONFIG_NAME': 'DAMSM',\n",
      " 'CUDA': True,\n",
      " 'DATASET_NAME': 'birds',\n",
      " 'DATA_DIR': '../data/birds/',\n",
      " 'GAN': {'B_ATTENTION': True,\n",
      "         'B_DCGAN': False,\n",
      "         'CONDITION_DIM': 100,\n",
      "         'DF_DIM': 64,\n",
      "         'GF_DIM': 128,\n",
      "         'R_NUM': 2,\n",
      "         'Z_DIM': 100},\n",
      " 'GPU_ID': 1,\n",
      " 'RNN_TYPE': 'LSTM',\n",
      " 'TEXT': {'CAPTIONS_PER_IMAGE': 10, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 18},\n",
      " 'TRAIN': {'BATCH_SIZE': 48,\n",
      "           'B_NET_D': True,\n",
      "           'DISCRIMINATOR_LR': 0.0002,\n",
      "           'ENCODER_LR': 0.002,\n",
      "           'FLAG': True,\n",
      "           'GENERATOR_LR': 0.0002,\n",
      "           'MAX_EPOCH': 600,\n",
      "           'NET_E': '',\n",
      "           'NET_G': '',\n",
      "           'RNN_GRAD_CLIP': 0.25,\n",
      "           'SMOOTH': {'GAMMA1': 4.0,\n",
      "                      'GAMMA2': 5.0,\n",
      "                      'GAMMA3': 10.0,\n",
      "                      'LAMBDA': 1.0},\n",
      "           'SNAPSHOT_INTERVAL': 50},\n",
      " 'TREE': {'BASE_SIZE': 299, 'BRANCH_NUM': 1},\n",
      " 'WORKERS': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/MyDataStor1/mmrl/AttnGAN/notebooks/miscc/config.py:104: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  yaml_cfg = edict(yaml.load(f))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = parse_args()\n",
    "if args.cfg_file is not None:\n",
    "    cfg_from_file(args.cfg_file)\n",
    "\n",
    "if args.gpu_id == -1:\n",
    "    cfg.CUDA = False\n",
    "else:\n",
    "    cfg.GPU_ID = args.gpu_id\n",
    "\n",
    "if args.data_dir != '':\n",
    "    cfg.DATA_DIR = args.data_dir\n",
    "print('Using config:')\n",
    "pprint.pprint(cfg)\n",
    "\n",
    "if not cfg.TRAIN.FLAG:\n",
    "    args.manualSeed = 100\n",
    "elif args.manualSeed is None:\n",
    "    args.manualSeed = random.randint(1, 10000)\n",
    "random.seed(args.manualSeed)\n",
    "np.random.seed(args.manualSeed)\n",
    "torch.manual_seed(args.manualSeed)\n",
    "if cfg.CUDA:\n",
    "    torch.cuda.manual_seed_all(args.manualSeed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "now = datetime.datetime.now(dateutil.tz.tzlocal())\n",
    "timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
    "output_dir = '../output/%s_%s_%s' % \\\n",
    "    (cfg.DATASET_NAME, cfg.CONFIG_NAME, timestamp)\n",
    "\n",
    "model_dir = os.path.join(output_dir, 'Model')\n",
    "image_dir = os.path.join(output_dir, 'Image')\n",
    "mkdir_p(model_dir)\n",
    "mkdir_p(image_dir)\n",
    "\n",
    "torch.cuda.set_device(cfg.GPU_ID)\n",
    "cudnn.benchmark = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mshaikh2/anaconda3/envs/pytorch-gpu/lib/python3.7/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total filenames:  11788 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n",
      "Load filenames from: ../data/birds//train/filenames.pickle (8855)\n",
      "Load filenames from: ../data/birds//test/filenames.pickle (2933)\n",
      "Load from:  ../data/birds/captions.pickle\n",
      "../data/birds/train\n",
      "../data/birds/train/class_info.pickle\n",
      "5450 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get data loader ##################################################\n",
    "imsize = cfg.TREE.BASE_SIZE * (2 ** (cfg.TREE.BRANCH_NUM-1))\n",
    "batch_size = cfg.TRAIN.BATCH_SIZE\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Scale(int(imsize * 76 / 64)),\n",
    "    transforms.RandomCrop(imsize),\n",
    "    transforms.RandomHorizontalFlip()])\n",
    "dataset = TextDataset(cfg.DATA_DIR, 'train',\n",
    "                      base_size=cfg.TREE.BASE_SIZE,\n",
    "                      transform=image_transform)\n",
    "\n",
    "print(dataset.n_words, dataset.embeddings_num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total filenames:  11788 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n",
      "Load filenames from: ../data/birds//train/filenames.pickle (8855)\n",
      "Load filenames from: ../data/birds//test/filenames.pickle (2933)\n",
      "Load from:  ../data/birds/captions.pickle\n",
      "../data/birds/test\n",
      "../data/birds/test/class_info.pickle\n"
     ]
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, drop_last=True,\n",
    "    shuffle=True, num_workers=int(cfg.WORKERS))\n",
    "\n",
    "# # validation data #\n",
    "dataset_val = TextDataset(cfg.DATA_DIR, 'test',\n",
    "                          base_size=cfg.TREE.BASE_SIZE,\n",
    "                          transform=image_transform)\n",
    "dataloader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=batch_size, drop_last=True,\n",
    "    shuffle=True, num_workers=int(cfg.WORKERS))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "UPDATE_INTERVAL = 2\n",
    "\n",
    "\n",
    "\n",
    "def train(dataloader, cnn_model, rnn_model, batch_size,\n",
    "          labels, optimizer, epoch, ixtoword, image_dir):\n",
    "    \n",
    "    cnn_model.train()\n",
    "    rnn_model.train()\n",
    "    s_total_loss0 = 0\n",
    "    s_total_loss1 = 0\n",
    "    w_total_loss0 = 0\n",
    "    w_total_loss1 = 0\n",
    "    count = (epoch + 1) * len(dataloader)\n",
    "    start_time = time.time()\n",
    "    for step, data in enumerate(dataloader, 0):\n",
    "        # print('step', step)\n",
    "        rnn_model.zero_grad()\n",
    "        cnn_model.zero_grad()\n",
    "\n",
    "        imgs, captions, cap_lens, \\\n",
    "            class_ids, keys = prepare_data(data)\n",
    "\n",
    "        # words_features: batch_size x nef x 17 x 17\n",
    "        # sent_code: batch_size x nef\n",
    "        words_features, sent_code = cnn_model(imgs[-1])\n",
    "        # --> batch_size x nef x 17*17\n",
    "#         print(words_features.shape,sent_code.shape)\n",
    "        nef, att_sze = words_features.size(1), words_features.size(2)\n",
    "        # words_features = words_features.view(batch_size, nef, -1)\n",
    "#         print(nef,att_sze)\n",
    "\n",
    "        hidden = rnn_model.init_hidden(batch_size)\n",
    "        # words_emb: batch_size x nef x seq_len\n",
    "        # sent_emb: batch_size x nef\n",
    "        words_emb, sent_emb = rnn_model(captions, cap_lens, hidden)\n",
    "#         print('here')\n",
    "        w_loss0, w_loss1, attn_maps = words_loss(words_features, words_emb, labels,\n",
    "                                                 cap_lens, class_ids, batch_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         print(w_loss0.data)\n",
    "#         print('--------------------------')\n",
    "#         print(w_loss1.data)\n",
    "#         print('--------------------------')\n",
    "#         print(attn_maps[0].shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "        w_total_loss0 += w_loss0.data\n",
    "        w_total_loss1 += w_loss1.data\n",
    "        loss = w_loss0 + w_loss1\n",
    "#         print(loss)\n",
    "        s_loss0, s_loss1 = \\\n",
    "            sent_loss(sent_code, sent_emb, labels, class_ids, batch_size)\n",
    "        loss += s_loss0 + s_loss1\n",
    "        \n",
    "        s_total_loss0 += s_loss0.data\n",
    "        s_total_loss1 += s_loss1.data\n",
    "        \n",
    "#         print(s_total_loss0[0],s_total_loss1[0])\n",
    "        #\n",
    "        loss.backward()\n",
    "        #\n",
    "        # `clip_grad_norm` helps prevent\n",
    "        # the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(rnn_model.parameters(),\n",
    "                                      cfg.TRAIN.RNN_GRAD_CLIP)\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % UPDATE_INTERVAL == 0:\n",
    "            count = epoch * len(dataloader) + step\n",
    "\n",
    "#             print(count)\n",
    "            s_cur_loss0 = s_total_loss0 / UPDATE_INTERVAL\n",
    "            s_cur_loss1 = s_total_loss1 / UPDATE_INTERVAL\n",
    "\n",
    "            w_cur_loss0 = w_total_loss0 / UPDATE_INTERVAL\n",
    "            w_cur_loss1 = w_total_loss1 / UPDATE_INTERVAL\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
    "                  's_loss {:5.2f} {:5.2f} | '\n",
    "                  'w_loss {:5.2f} {:5.2f}'\n",
    "                  .format(epoch, step, len(dataloader),\n",
    "                          elapsed * 1000. / UPDATE_INTERVAL,\n",
    "                          s_cur_loss0, s_cur_loss1,\n",
    "                          w_cur_loss0, w_cur_loss1))\n",
    "            s_total_loss0 = 0\n",
    "            s_total_loss1 = 0\n",
    "            w_total_loss0 = 0\n",
    "            w_total_loss1 = 0\n",
    "            start_time = time.time()\n",
    "            # attention Maps\n",
    "            \n",
    "#             print(imgs[-1].cpu().shape, captions.shape, len(attn_maps),attn_maps[-1].shape, att_sze)\n",
    "            img_set, _ = \\\n",
    "                build_super_images(imgs[-1].cpu(), captions,\n",
    "                                   ixtoword, attn_maps, att_sze)\n",
    "            if img_set is not None:\n",
    "                im = Image.fromarray(img_set)\n",
    "                fullpath = '%s/attention_maps%d.png' % (image_dir, step)\n",
    "                im.save(fullpath)\n",
    "    return count\n",
    "\n",
    "\n",
    "def evaluate(dataloader, cnn_model, rnn_model, batch_size):\n",
    "    cnn_model.eval()\n",
    "    rnn_model.eval()\n",
    "    s_total_loss = 0\n",
    "    w_total_loss = 0\n",
    "    for step, data in enumerate(dataloader, 0):\n",
    "        real_imgs, captions, cap_lens, \\\n",
    "                class_ids, keys = prepare_data(data)\n",
    "\n",
    "        words_features, sent_code = cnn_model(real_imgs[-1])\n",
    "        # nef = words_features.size(1)\n",
    "        # words_features = words_features.view(batch_size, nef, -1)\n",
    "\n",
    "        hidden = rnn_model.init_hidden(batch_size)\n",
    "        words_emb, sent_emb = rnn_model(captions, cap_lens, hidden)\n",
    "\n",
    "        w_loss0, w_loss1, attn = words_loss(words_features, words_emb, labels,\n",
    "                                            cap_lens, class_ids, batch_size)\n",
    "        w_total_loss += (w_loss0 + w_loss1).data\n",
    "\n",
    "        s_loss0, s_loss1 = \\\n",
    "            sent_loss(sent_code, sent_emb, labels, class_ids, batch_size)\n",
    "        s_total_loss += (s_loss0 + s_loss1).data\n",
    "\n",
    "        if step == 50:\n",
    "            break\n",
    "\n",
    "    s_cur_loss = s_total_loss[0] / step\n",
    "    w_cur_loss = w_total_loss[0] / step\n",
    "\n",
    "    return s_cur_loss, w_cur_loss\n",
    "\n",
    "\n",
    "def build_models():\n",
    "    # build model ############################################################\n",
    "    text_encoder = RNN_ENCODER(dataset.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
    "    image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n",
    "    labels = Variable(torch.LongTensor(range(batch_size)))\n",
    "    start_epoch = 0\n",
    "    if cfg.TRAIN.NET_E != '':\n",
    "        state_dict = torch.load(cfg.TRAIN.NET_E)\n",
    "        text_encoder.load_state_dict(state_dict)\n",
    "        print('Load ', cfg.TRAIN.NET_E)\n",
    "        #\n",
    "        name = cfg.TRAIN.NET_E.replace('text_encoder', 'image_encoder')\n",
    "        state_dict = torch.load(name)\n",
    "        image_encoder.load_state_dict(state_dict)\n",
    "        print('Load ', name)\n",
    "\n",
    "        istart = cfg.TRAIN.NET_E.rfind('_') + 8\n",
    "        iend = cfg.TRAIN.NET_E.rfind('.')\n",
    "        start_epoch = cfg.TRAIN.NET_E[istart:iend]\n",
    "        start_epoch = int(start_epoch) + 1\n",
    "        print('start_epoch', start_epoch)\n",
    "    if cfg.CUDA:\n",
    "        text_encoder = text_encoder.cuda()\n",
    "        image_encoder = image_encoder.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "    return text_encoder, image_encoder, labels, start_epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mshaikh2/anaconda3/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained model from  https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\n"
     ]
    }
   ],
   "source": [
    "# Train ##############################################################\n",
    "text_encoder, image_encoder, labels, start_epoch = build_models()\n",
    "para = list(text_encoder.parameters())\n",
    "for v in image_encoder.parameters():\n",
    "    if v.requires_grad:\n",
    "        para.append(v)\n",
    "# optimizer = optim.Adam(para, lr=cfg.TRAIN.ENCODER_LR, betas=(0.5, 0.999))\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "\n",
    "lr = cfg.TRAIN.ENCODER_LR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def words_loss(img_features, words_emb, labels,\n",
    "#                cap_lens, class_ids, batch_size):\n",
    "#     \"\"\"\n",
    "#         words_emb(query): batch x nef x seq_len\n",
    "#         img_features(context): batch x nef x 17 x 17\n",
    "#     \"\"\"\n",
    "#     masks = []\n",
    "#     att_maps = []\n",
    "#     similarities = []\n",
    "#     cap_lens = cap_lens.data.tolist()\n",
    "#     for i in range(batch_size):\n",
    "#         if class_ids is not None:\n",
    "#             print('class_ids:',class_ids)\n",
    "#             mask = (class_ids == class_ids[i]).astype(np.bool)\n",
    "#             print('mask:',mask)\n",
    "#             mask[i] = 0\n",
    "#             masks.append(mask.reshape((1, -1)))\n",
    "            \n",
    "#         # Get the i-th text description\n",
    "#         words_num = cap_lens[i]\n",
    "#         # -> 1 x nef x words_num\n",
    "#         word = words_emb[i, :, :words_num].unsqueeze(0).contiguous()\n",
    "#         # -> batch_size x nef x words_num\n",
    "#         word = word.repeat(batch_size, 1, 1)\n",
    "#         # batch x nef x 17*17\n",
    "#         context = img_features\n",
    "#         \"\"\"\n",
    "#             word(query): batch x nef x words_num\n",
    "#             context: batch x nef x 17 x 17\n",
    "#             weiContext: batch x nef x words_num\n",
    "#             attn: batch x words_num x 17 x 17\n",
    "#         \"\"\"\n",
    "#         weiContext, attn = func_attention(word, context, cfg.TRAIN.SMOOTH.GAMMA1)\n",
    "#         att_maps.append(attn[i].unsqueeze(0).contiguous())\n",
    "#         # --> batch_size x words_num x nef\n",
    "#         word = word.transpose(1, 2).contiguous()\n",
    "#         weiContext = weiContext.transpose(1, 2).contiguous()\n",
    "#         # --> batch_size*words_num x nef\n",
    "#         word = word.view(batch_size * words_num, -1)\n",
    "#         weiContext = weiContext.view(batch_size * words_num, -1)\n",
    "#         #\n",
    "#         # -->batch_size*words_num\n",
    "#         row_sim = cosine_similarity(word, weiContext)\n",
    "#         # --> batch_size x words_num\n",
    "#         row_sim = row_sim.view(batch_size, words_num)\n",
    "\n",
    "#         # Eq. (10)\n",
    "#         row_sim.mul_(cfg.TRAIN.SMOOTH.GAMMA2).exp_()\n",
    "#         row_sim = row_sim.sum(dim=1, keepdim=True)\n",
    "#         row_sim = torch.log(row_sim)\n",
    "\n",
    "#         # --> 1 x batch_size\n",
    "#         # similarities(i, j): the similarity between the i-th image and the j-th text description\n",
    "#         similarities.append(row_sim)\n",
    "\n",
    "#     # batch_size x batch_size\n",
    "#     similarities = torch.cat(similarities, 1)\n",
    "#     if class_ids is not None:\n",
    "#         masks = np.concatenate(masks, 0)\n",
    "#         # masks: batch_size x batch_size\n",
    "#         masks = torch.ByteTensor(masks)\n",
    "#         if cfg.CUDA:\n",
    "#             masks = masks.cuda()\n",
    "\n",
    "#     similarities = similarities * cfg.TRAIN.SMOOTH.GAMMA3\n",
    "#     if class_ids is not None:\n",
    "#         similarities.data.masked_fill_(masks, -float('inf'))\n",
    "#     similarities1 = similarities.transpose(0, 1)\n",
    "#     if labels is not None:\n",
    "#         loss0 = nn.CrossEntropyLoss()(similarities, labels)\n",
    "#         loss1 = nn.CrossEntropyLoss()(similarities1, labels)\n",
    "#     else:\n",
    "#         loss0, loss1 = None, None\n",
    "#     return loss0, loss1, att_maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import errno\n",
    "# import numpy as np\n",
    "# from torch.nn import init\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# from PIL import Image, ImageDraw, ImageFont\n",
    "# from copy import deepcopy\n",
    "# import skimage.transform\n",
    "# # For visualization ################################################\n",
    "# COLOR_DIC = {0:[128,64,128],  1:[244, 35,232],\n",
    "#              2:[70, 70, 70],  3:[102,102,156],\n",
    "#              4:[190,153,153], 5:[153,153,153],\n",
    "#              6:[250,170, 30], 7:[220, 220, 0],\n",
    "#              8:[107,142, 35], 9:[152,251,152],\n",
    "#              10:[70,130,180], 11:[220,20, 60],\n",
    "#              12:[255, 0, 0],  13:[0, 0, 142],\n",
    "#              14:[119,11, 32], 15:[0, 60,100],\n",
    "#              16:[0, 80, 100], 17:[0, 0, 230],\n",
    "#              18:[0,  0, 70],  19:[0, 0,  0]}\n",
    "# FONT_MAX = 50\n",
    "# def build_super_images(real_imgs, captions, ixtoword,\n",
    "#                        attn_maps, att_sze, lr_imgs=None,\n",
    "#                        batch_size=cfg.TRAIN.BATCH_SIZE,\n",
    "#                        max_word_num=cfg.TEXT.WORDS_NUM):\n",
    "#     nvis = 8\n",
    "#     real_imgs = real_imgs[:nvis]\n",
    "#     if lr_imgs is not None:\n",
    "#         lr_imgs = lr_imgs[:nvis]\n",
    "#     if att_sze == 17:\n",
    "#         vis_size = att_sze * 16\n",
    "#     else:\n",
    "#         vis_size = real_imgs.size(2)\n",
    "\n",
    "# #     print('vis_size:',vis_size)\n",
    "#     text_convas = \\\n",
    "#         np.ones([batch_size * FONT_MAX,\n",
    "#                  (max_word_num + 2) * (vis_size + 2), 3],\n",
    "#                 dtype=np.uint8)\n",
    "\n",
    "#     for i in range(max_word_num):\n",
    "#         istart = (i + 2) * (vis_size + 2)\n",
    "#         iend = (i + 3) * (vis_size + 2)\n",
    "#         text_convas[:, istart:iend, :] = COLOR_DIC[i]\n",
    "\n",
    "\n",
    "#     real_imgs = \\\n",
    "#         nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(real_imgs)\n",
    "#     # [-1, 1] --> [0, 1]\n",
    "#     real_imgs.add_(1).div_(2).mul_(255)\n",
    "#     real_imgs = real_imgs.data.numpy()\n",
    "#     # b x c x h x w --> b x h x w x c\n",
    "#     real_imgs = np.transpose(real_imgs, (0, 2, 3, 1))\n",
    "#     pad_sze = real_imgs.shape\n",
    "#     middle_pad = np.zeros([pad_sze[2], 2, 3])\n",
    "#     post_pad = np.zeros([pad_sze[1], pad_sze[2], 3])\n",
    "#     if lr_imgs is not None:\n",
    "#         lr_imgs = \\\n",
    "#             nn.Upsample(size=(vis_size, vis_size), mode='bilinear')(lr_imgs)\n",
    "#         # [-1, 1] --> [0, 1]\n",
    "#         lr_imgs.add_(1).div_(2).mul_(255)\n",
    "#         lr_imgs = lr_imgs.data.numpy()\n",
    "#         # b x c x h x w --> b x h x w x c\n",
    "#         lr_imgs = np.transpose(lr_imgs, (0, 2, 3, 1))\n",
    "\n",
    "#     # batch x seq_len x 17 x 17 --> batch x 1 x 17 x 17\n",
    "#     seq_len = max_word_num\n",
    "#     img_set = []\n",
    "#     num = nvis  # len(attn_maps)\n",
    "# #     print('num:',num)\n",
    "#     text_map, sentences = \\\n",
    "#         drawCaption(text_convas, captions, ixtoword, vis_size)\n",
    "#     text_map = np.asarray(text_map).astype(np.uint8)\n",
    "\n",
    "#     bUpdate = 1\n",
    "#     for i in range(num): #num: 8\n",
    "# #         print('1 attn_maps[i].shape:',attn_maps[i].shape)\n",
    "#         attn = attn_maps[i].cpu().view(1, -1, att_sze, att_sze)\n",
    "# #         print('2 attn.shape:',attn.shape)\n",
    "#         # --> 1 x 1 x 17 x 17\n",
    "#         attn_max = attn.max(dim=1, keepdim=True)\n",
    "#         attn = torch.cat([attn_max[0], attn], 1)\n",
    "# #         print('3 attn.shape:',attn.shape)\n",
    "#         #\n",
    "#         attn = attn.view(-1, 1, att_sze, att_sze)\n",
    "# #         print('4 attn.shape:',attn.shape)\n",
    "#         attn = attn.repeat(1, 3, 1, 1).data.numpy()\n",
    "# #         print('5 attn.shape:',attn.shape)\n",
    "#         # n x c x h x w --> n x h x w x c\n",
    "#         attn = np.transpose(attn, (0, 2, 3, 1))\n",
    "# #         print('6 attn.shape:',attn.shape)\n",
    "#         num_attn = attn.shape[0]\n",
    "# #         print('num_attn:',num_attn)\n",
    "#         #\n",
    "#         img = real_imgs[i]\n",
    "#         if lr_imgs is None:\n",
    "#             lrI = img\n",
    "#         else:\n",
    "#             lrI = lr_imgs[i]\n",
    "#         row = [lrI, middle_pad]\n",
    "#         row_merge = [img, middle_pad]\n",
    "#         row_beforeNorm = []\n",
    "#         minVglobal, maxVglobal = 1, 0\n",
    "#         for j in range(num_attn):\n",
    "# #             print('attn.shape:',attn.shape)\n",
    "#             one_map = attn[j]\n",
    "# #             print('0 one_map.shape:',one_map.shape)\n",
    "#             if (vis_size // att_sze) > 1:\n",
    "#                 one_map = skimage.transform.pyramid_expand(one_map, sigma=20\n",
    "#                                                            ,upscale=vis_size // att_sze\n",
    "#                                                            ,multichannel=True)\n",
    "# #             print('1 one_map.shape:',one_map.shape)\n",
    "#             row_beforeNorm.append(one_map)\n",
    "#             minV = one_map.min()\n",
    "#             maxV = one_map.max()\n",
    "#             if minVglobal > minV:\n",
    "#                 minVglobal = minV\n",
    "#             if maxVglobal < maxV:\n",
    "#                 maxVglobal = maxV\n",
    "# #         print('len(row_beforeNorm):',len(row_beforeNorm))\n",
    "#         for j in range(seq_len + 1):\n",
    "#             if j < num_attn:\n",
    "# #                 print('2 one_map.shape:',one_map.shape)\n",
    "#                 one_map = row_beforeNorm[j]\n",
    "# #                 print('3 one_map.shape:',one_map.shape)\n",
    "#                 one_map = (one_map - minVglobal) / (maxVglobal - minVglobal)\n",
    "#                 one_map *= 255\n",
    "# #                 print('4 one_map.shape:',one_map.shape)\n",
    "#                 #\n",
    "                \n",
    "                \n",
    "# #                 print(np.uint8(one_map).shape,type(np.uint8(one_map)))\n",
    "                \n",
    "                \n",
    "# #                 print(img.shape,one_map.shape)\n",
    "#                 PIL_im = Image.fromarray(np.uint8(img))\n",
    "# #                 display(PIL_im)\n",
    "#                 PIL_att = Image.fromarray(np.uint8(one_map))\n",
    "#                 merged = \\\n",
    "#                     Image.new('RGBA', (vis_size, vis_size), (0, 0, 0, 0))\n",
    "#                 mask = Image.new('L', (vis_size, vis_size), (210))\n",
    "# #                 print('---------------')\n",
    "# #                 print(np.asarray(mask))\n",
    "#                 merged.paste(PIL_im, (0, 0))\n",
    "                \n",
    "                \n",
    "#                 merged.paste(PIL_att, (0, 0), mask)\n",
    "#                 merged = np.array(merged)[:, :, :3]\n",
    "#             else:\n",
    "#                 one_map = post_pad\n",
    "#                 merged = post_pad\n",
    "#             row.append(one_map)\n",
    "#             row.append(middle_pad)\n",
    "#             #\n",
    "#             row_merge.append(merged)\n",
    "#             row_merge.append(middle_pad)\n",
    "#         row = np.concatenate(row, 1)\n",
    "#         row_merge = np.concatenate(row_merge, 1)\n",
    "#         txt = text_map[i * FONT_MAX: (i + 1) * FONT_MAX]\n",
    "#         if txt.shape[1] != row.shape[1]:\n",
    "# #             print('txt', txt.shape, 'row', row.shape)\n",
    "#             bUpdate = 0\n",
    "#             break\n",
    "#         row = np.concatenate([txt, row, row_merge], 0)\n",
    "#         img_set.append(row)\n",
    "#     if bUpdate:\n",
    "#         img_set = np.concatenate(img_set, 0)\n",
    "#         img_set = img_set.astype(np.uint8)\n",
    "#         return img_set, sentences\n",
    "#     else:\n",
    "#         return None\n",
    "# def drawCaption(convas, captions, ixtoword, vis_size, off1=2, off2=2):\n",
    "#     num = captions.size(0)\n",
    "#     img_txt = Image.fromarray(convas)\n",
    "#     # get a font\n",
    "#     # fnt = None  # ImageFont.truetype('Pillow/Tests/fonts/FreeMono.ttf', 50)\n",
    "#     fnt = ImageFont.truetype('Pillow/Tests/fonts/FreeMono.ttf', 50)\n",
    "#     # get a drawing context\n",
    "#     d = ImageDraw.Draw(img_txt)\n",
    "#     sentence_list = []\n",
    "#     for i in range(num):\n",
    "#         cap = captions[i].data.cpu().numpy()\n",
    "#         sentence = []\n",
    "#         for j in range(len(cap)):\n",
    "#             if cap[j] == 0:\n",
    "#                 break\n",
    "#             word = ixtoword[cap[j]].encode('ascii', 'ignore').decode('ascii')\n",
    "#             d.text(((j + off1) * (vis_size + off2), i * FONT_MAX), '%d:%s' % (j, word[:6]),\n",
    "#                    font=fnt, fill=(255, 255, 255, 255))\n",
    "#             sentence.append(word)\n",
    "#         sentence_list.append(sentence)\n",
    "#     return img_txt, sentence_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mshaikh2/anaconda3/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/functional.py:2479: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |     0/  184 batches | ms/batch 328.79 | s_loss  1.95  1.96 | w_loss  2.78  2.24\n",
      "| epoch   0 |     2/  184 batches | ms/batch 6115.63 | s_loss  3.92  3.91 | w_loss  5.30  4.28\n",
      "| epoch   0 |     4/  184 batches | ms/batch 5972.23 | s_loss  3.90  3.90 | w_loss  4.73  3.98\n",
      "| epoch   0 |     6/  184 batches | ms/batch 6043.24 | s_loss  3.88  3.89 | w_loss  4.65  3.96\n",
      "| epoch   0 |     8/  184 batches | ms/batch 6008.20 | s_loss  3.87  3.87 | w_loss  4.36  3.97\n",
      "| epoch   0 |    10/  184 batches | ms/batch 5968.83 | s_loss  3.86  3.87 | w_loss  4.25  3.94\n",
      "| epoch   0 |    12/  184 batches | ms/batch 5741.01 | s_loss  3.86  3.87 | w_loss  4.05  3.90\n",
      "| epoch   0 |    14/  184 batches | ms/batch 5822.53 | s_loss  3.86  3.87 | w_loss  3.95  3.91\n",
      "| epoch   0 |    16/  184 batches | ms/batch 5874.10 | s_loss  3.86  3.87 | w_loss  3.92  3.89\n",
      "| epoch   0 |    18/  184 batches | ms/batch 5872.86 | s_loss  3.86  3.87 | w_loss  3.88  3.90\n",
      "| epoch   0 |    20/  184 batches | ms/batch 6025.45 | s_loss  3.85  3.86 | w_loss  3.89  3.86\n",
      "| epoch   0 |    22/  184 batches | ms/batch 6040.94 | s_loss  3.86  3.87 | w_loss  3.85  3.85\n",
      "| epoch   0 |    24/  184 batches | ms/batch 5814.61 | s_loss  3.85  3.86 | w_loss  3.87  3.86\n",
      "| epoch   0 |    26/  184 batches | ms/batch 5853.15 | s_loss  3.82  3.83 | w_loss  3.86  3.82\n",
      "| epoch   0 |    28/  184 batches | ms/batch 5714.06 | s_loss  3.80  3.82 | w_loss  3.91  3.85\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b31d664a0bc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     count = train(dataloader, image_encoder, text_encoder,\n\u001b[1;32m      5\u001b[0m                   \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                   dataset.ixtoword, image_dir)\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m89\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-eaf2a1679850>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, cnn_model, rnn_model, batch_size, labels, optimizer, epoch, ixtoword, image_dir)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mimg_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 build_super_images(imgs[-1].cpu(), captions,\n\u001b[0;32m--> 101\u001b[0;31m                                    ixtoword, attn_maps, att_sze)\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimg_set\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-fad7da3a6bf4>\u001b[0m in \u001b[0;36mbuild_super_images\u001b[0;34m(real_imgs, captions, ixtoword, attn_maps, att_sze, lr_imgs, batch_size, max_word_num)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 one_map = skimage.transform.pyramid_expand(one_map, sigma=20\n\u001b[1;32m    112\u001b[0m                                                            \u001b[0;34m,\u001b[0m\u001b[0mupscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvis_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0matt_sze\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                                                            ,multichannel=True)\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;31m#             print('1 one_map.shape:',one_map.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mrow_beforeNorm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-gpu/lib/python3.7/site-packages/skimage/transform/pyramids.py\u001b[0m in \u001b[0;36mpyramid_expand\u001b[0;34m(image, upscale, sigma, order, mode, cval, multichannel)\u001b[0m\n\u001b[1;32m    128\u001b[0m     resized = resize(image, out_shape, order=order,\n\u001b[1;32m    129\u001b[0m                      mode=mode, cval=cval, anti_aliasing=False)\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_smooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultichannel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-gpu/lib/python3.7/site-packages/skimage/transform/pyramids.py\u001b[0m in \u001b[0;36m_smooth\u001b[0;34m(image, sigma, mode, cval, multichannel)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     ndi.gaussian_filter(image, sigma, output=smoothed,\n\u001b[0;32m---> 16\u001b[0;31m                         mode=mode, cval=cval)\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msmoothed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-gpu/lib/python3.7/site-packages/scipy/ndimage/filters.py\u001b[0m in \u001b[0;36mgaussian_filter\u001b[0;34m(input, sigma, order, output, mode, cval, truncate)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             gaussian_filter1d(input, sigma, axis, order, output,\n\u001b[0;32m--> 299\u001b[0;31m                               mode, cval, truncate)\n\u001b[0m\u001b[1;32m    300\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-gpu/lib/python3.7/site-packages/scipy/ndimage/filters.py\u001b[0m in \u001b[0;36mgaussian_filter1d\u001b[0;34m(input, sigma, axis, order, output, mode, cval, truncate)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;31m# Since we are calling correlate, not convolve, revert the kernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_gaussian_kernel1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorrelate1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-gpu/lib/python3.7/site-packages/scipy/ndimage/filters.py\u001b[0m in \u001b[0;36mcorrelate1d\u001b[0;34m(input, weights, axis, output, mode, cval, origin)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ni_support\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_mode_to_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     _nd_image.correlate1d(input, weights, axis, output, mode, cval,\n\u001b[0;32m---> 95\u001b[0;31m                           origin)\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, cfg.TRAIN.MAX_EPOCH):\n",
    "    optimizer = optim.Adam(para, lr=lr, betas=(0.5, 0.999))\n",
    "    epoch_start_time = time.time()\n",
    "    count = train(dataloader, image_encoder, text_encoder,\n",
    "                  batch_size, labels, optimizer, epoch,\n",
    "                  dataset.ixtoword, image_dir)\n",
    "    print('-' * 89)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(dataloader_val) > 0:\n",
    "        s_loss, w_loss = evaluate(dataloader_val, image_encoder,\n",
    "                                  text_encoder, batch_size)\n",
    "        print('| end epoch {:3d} | valid loss '\n",
    "              '{:5.2f} {:5.2f} | lr {:.5f}|'\n",
    "              .format(epoch, s_loss, w_loss, lr))\n",
    "    print('-' * 89)\n",
    "    if lr > cfg.TRAIN.ENCODER_LR/10.:\n",
    "        lr *= 0.98\n",
    "\n",
    "    if (epoch % cfg.TRAIN.SNAPSHOT_INTERVAL == 0 or\n",
    "        epoch == cfg.TRAIN.MAX_EPOCH):\n",
    "        torch.save(image_encoder.state_dict(),\n",
    "                   '%s/image_encoder%d.pth' % (model_dir, epoch))\n",
    "        torch.save(text_encoder.state_dict(),\n",
    "                   '%s/text_encoder%d.pth' % (model_dir, epoch))\n",
    "        print('Save G/Ds models.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-gpu)",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
