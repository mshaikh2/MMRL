{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from miscc.utils import mkdir_p\n",
    "from miscc.utils import build_super_images\n",
    "from miscc.losses import sent_loss, words_loss\n",
    "from miscc.config import cfg, cfg_from_file\n",
    "\n",
    "from datasets import TextDataset\n",
    "from datasets import prepare_data\n",
    "\n",
    "from model import RNN_ENCODER, CNN_ENCODER\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import pprint\n",
    "import datetime\n",
    "import dateutil.tz\n",
    "import argparse\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "UPDATE_INTERVAL = 100\n",
    "class parse_args():\n",
    "    cfg_file='../code/cfg/DAMSM/coco.yml'\n",
    "    gpu_id=3\n",
    "    data_dir='../data/coco/'\n",
    "    manualSeed=1\n",
    "args = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(dataloader, cnn_model, trx_model, batch_size,\n",
    "          labels, optimizer, epoch, ixtoword, image_dir):\n",
    "    \n",
    "    cnn_model.train()\n",
    "    trx_model.train()\n",
    "    s_total_loss0 = 0\n",
    "    s_total_loss1 = 0\n",
    "    w_total_loss0 = 0\n",
    "    w_total_loss1 = 0\n",
    "    count = (epoch + 1) * len(dataloader)\n",
    "    start_time = time.time()\n",
    "    for step, data in enumerate(dataloader, 0):\n",
    "        # print('step', step)\n",
    "        trx_model.zero_grad()\n",
    "        cnn_model.zero_grad()\n",
    "\n",
    "        imgs, captions, cap_lens, \\\n",
    "            class_ids, keys = prepare_data(data)\n",
    "\n",
    "        # words_features: batch_size x nef x 17 x 17\n",
    "        # sent_code: batch_size x nef\n",
    "        words_features, sent_code = cnn_model(imgs[-1])\n",
    "        # --> batch_size x nef x 17*17\n",
    "#         print(words_features.shape,sent_code.shape)\n",
    "        nef, att_sze = words_features.size(1), words_features.size(2)\n",
    "        # words_features = words_features.view(batch_size, nef, -1)\n",
    "#         print('nef:{0},att_sze:{1}'.format(nef,att_sze))\n",
    "\n",
    "#         hidden = trx_model.init_hidden(batch_size)\n",
    "        # words_emb: batch_size x nef x seq_len\n",
    "        # sent_emb: batch_size x nef\n",
    "#         print('captions:',captions, captions.size())\n",
    "        \n",
    "        # words_emb: batch_size x nef x seq_len\n",
    "        # sent_emb: batch_size x nef\n",
    "#         words_emb, sent_emb = trx_model(captions, cap_lens, hidden)\n",
    "        \n",
    "        words_emb, sent_emb = trx_model(captions)\n",
    "#         print('words_emb:',words_emb.size(),', sent_emb:', sent_emb.size())\n",
    "        w_loss0, w_loss1, attn_maps = words_loss(words_features, words_emb, labels,\n",
    "                                                 cap_lens, class_ids, batch_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         print(w_loss0.data)\n",
    "#         print('--------------------------')\n",
    "#         print(w_loss1.data)\n",
    "#         print('--------------------------')\n",
    "#         print(attn_maps[0].shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "        w_total_loss0 += w_loss0.data\n",
    "        w_total_loss1 += w_loss1.data\n",
    "        loss = w_loss0 + w_loss1\n",
    "#         print(loss)\n",
    "        s_loss0, s_loss1 = \\\n",
    "            sent_loss(sent_code, sent_emb, labels, class_ids, batch_size)\n",
    "        loss += s_loss0 + s_loss1\n",
    "        \n",
    "        s_total_loss0 += s_loss0.data\n",
    "        s_total_loss1 += s_loss1.data\n",
    "        \n",
    "#         print(s_total_loss0[0],s_total_loss1[0])\n",
    "        #\n",
    "        loss.backward()\n",
    "        #\n",
    "        # `clip_grad_norm` helps prevent\n",
    "        # the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(trx_model.parameters(),\n",
    "                                      cfg.TRAIN.RNN_GRAD_CLIP)\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % UPDATE_INTERVAL == 0:\n",
    "            count = epoch * len(dataloader) + step\n",
    "\n",
    "#             print(count)\n",
    "            s_cur_loss0 = s_total_loss0 / UPDATE_INTERVAL\n",
    "            s_cur_loss1 = s_total_loss1 / UPDATE_INTERVAL\n",
    "\n",
    "            w_cur_loss0 = w_total_loss0 / UPDATE_INTERVAL\n",
    "            w_cur_loss1 = w_total_loss1 / UPDATE_INTERVAL\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
    "                  's_loss {:5.2f} {:5.2f} | '\n",
    "                  'w_loss {:5.2f} {:5.2f}'\n",
    "                  .format(epoch, step, len(dataloader),\n",
    "                          elapsed * 1000. / UPDATE_INTERVAL,\n",
    "                          s_cur_loss0, s_cur_loss1,\n",
    "                          w_cur_loss0, w_cur_loss1))\n",
    "            tbw.add_scalar('train_w_loss0', float(w_cur_loss0.item()), epoch)\n",
    "            tbw.add_scalar('train_s_loss0', float(s_cur_loss0.item()), epoch)\n",
    "            tbw.add_scalar('train_w_loss1', float(w_cur_loss1.item()), epoch)\n",
    "            tbw.add_scalar('train_s_loss1', float(s_cur_loss1.item()), epoch)\n",
    "            s_total_loss0 = 0\n",
    "            s_total_loss1 = 0\n",
    "            w_total_loss0 = 0\n",
    "            w_total_loss1 = 0\n",
    "            start_time = time.time()\n",
    "            # attention Maps\n",
    "            \n",
    "#             print(imgs[-1].cpu().shape, captions.shape, len(attn_maps),attn_maps[-1].shape, att_sze)\n",
    "            img_set, _ = \\\n",
    "                build_super_images(imgs[-1].cpu(), captions, ixtoword, attn_maps, att_sze)\n",
    "            if img_set is not None:\n",
    "                im = Image.fromarray(img_set)\n",
    "                fullpath = '{0}/attention_maps_e{1}_s{2}.png'.format(image_dir,epoch, step)\n",
    "                im.save(fullpath)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(dataloader, cnn_model, rnn_model, batch_size):\n",
    "    cnn_model.eval()\n",
    "    rnn_model.eval()\n",
    "    s_total_loss = 0\n",
    "    w_total_loss = 0\n",
    "    for step, data in enumerate(dataloader, 0):\n",
    "        real_imgs, captions, cap_lens, \\\n",
    "                class_ids, keys = prepare_data(data)\n",
    "\n",
    "        words_features, sent_code = cnn_model(real_imgs[-1])\n",
    "        # nef = words_features.size(1)\n",
    "        # words_features = words_features.view(batch_size, nef, -1)\n",
    "\n",
    "        hidden = rnn_model.init_hidden(batch_size)\n",
    "        words_emb, sent_emb = rnn_model(captions, cap_lens, hidden)\n",
    "\n",
    "        w_loss0, w_loss1, attn = words_loss(words_features, words_emb, labels,\n",
    "                                            cap_lens, class_ids, batch_size)\n",
    "        w_total_loss += (w_loss0 + w_loss1).data\n",
    "\n",
    "        s_loss0, s_loss1 = \\\n",
    "            sent_loss(sent_code, sent_emb, labels, class_ids, batch_size)\n",
    "        s_total_loss += (s_loss0 + s_loss1).data\n",
    "\n",
    "        if step == 50:\n",
    "            break\n",
    "\n",
    "    s_cur_loss = s_total_loss / step\n",
    "    w_cur_loss = w_total_loss / step\n",
    "\n",
    "    return s_cur_loss, w_cur_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "################ Transformer: Text Encoder ############\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, k, heads=8):\n",
    "        super().__init__()\n",
    "        self.k, self.heads = k, heads\n",
    "        # These compute the queries, keys and values for all \n",
    "        # heads (as a single concatenated vector)\n",
    "        self.tokeys    = nn.Linear(k, k * heads, bias=False)\n",
    "        self.toqueries = nn.Linear(k, k * heads, bias=False)\n",
    "        self.tovalues  = nn.Linear(k, k * heads, bias=False)\n",
    "\n",
    "        # This unifies the outputs of the different heads into \n",
    "        # a single k-vector\n",
    "        self.unifyheads = nn.Linear(heads * k, k)\n",
    "    def forward(self, x):\n",
    "        b, t, k = x.size()\n",
    "        h = self.heads\n",
    "\n",
    "        queries = self.toqueries(x).view(b, t, h, k)\n",
    "        keys    = self.tokeys(x)   .view(b, t, h, k)\n",
    "        values  = self.tovalues(x) .view(b, t, h, k)\n",
    "        \n",
    "        keys = keys.transpose(1, 2).contiguous().view(b * h, t, k)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(b * h, t, k)\n",
    "        values = values.transpose(1, 2).contiguous().view(b * h, t, k)\n",
    "        \n",
    "        queries = queries / (k ** (1/4))\n",
    "        keys    = keys / (k ** (1/4))\n",
    "\n",
    "        # - get dot product of queries and keys, and scale\n",
    "        dot = torch.bmm(queries, keys.transpose(1, 2))\n",
    "        # - dot has size (b*h, t, t) containing raw weights\n",
    "\n",
    "        dot = F.softmax(dot, dim=2) \n",
    "        # - dot now contains row-wise normalized weights\n",
    "        \n",
    "        # apply the self attention to the values\n",
    "        out = torch.bmm(dot, values).view(b, h, t, k)\n",
    "        \n",
    "        out = out.transpose(1, 2).contiguous().view(b, t, h * k)\n",
    "        return self.unifyheads(out)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, emb, heads, mask, seq_length, ff_hidden_mult=4, dropout=0.0, wide=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = SelfAttention(k=emb, heads=heads)\n",
    "        \n",
    "        self.mask = mask\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(emb)\n",
    "        self.norm2 = nn.LayerNorm(emb)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(emb, ff_hidden_mult * emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_mult * emb, emb)\n",
    "        )\n",
    "\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        attended = self.attention(x)\n",
    "\n",
    "        x = self.norm1(attended + x)\n",
    "\n",
    "        x = self.do(x)\n",
    "\n",
    "        fedforward = self.ff(x)\n",
    "\n",
    "        x = self.norm2(fedforward + x)\n",
    "\n",
    "        x = self.do(x)\n",
    "\n",
    "        return x\n",
    "class TEXT_TRANSFORMER_ENCODER(nn.Module):\n",
    "    def __init__(self, emb, heads, depth, seq_length, num_tokens, dropout=0.0, wide=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_tokens = num_tokens\n",
    "\n",
    "        self.token_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=num_tokens)\n",
    "        self.pos_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=seq_length)\n",
    "\n",
    "        tblocks = []\n",
    "        for i in range(depth):\n",
    "            tblocks.append(\n",
    "                TransformerBlock(emb=emb\n",
    "                                 , heads=heads\n",
    "                                 , seq_length=seq_length\n",
    "                                 , mask=False\n",
    "                                 , dropout=dropout\n",
    "                                 , wide=wide))\n",
    "\n",
    "        self.tblocks = nn.Sequential(*tblocks)\n",
    "\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: A batch by sequence length integer tensor of token indices.\n",
    "        :return: predicted log-probability vectors for each token based on the preceding tokens.\n",
    "        \"\"\"\n",
    "        tokens = self.token_embedding(x)\n",
    "        \n",
    "        \n",
    "        b, t, e = tokens.size()\n",
    "#         print('b:{0}, t:{1}, e:{2}'.format(b, t, e))\n",
    "        positions = torch.arange(t,device='cuda')\n",
    "        \n",
    "        positions = self.pos_embedding(positions)[None, :, :].expand(b, t, e)\n",
    "        \n",
    "#         print('positions:',positions.size())\n",
    "        x = tokens + positions\n",
    "        x = self.do(x)\n",
    "\n",
    "#         print('x:',x.size())\n",
    "        words_emb = self.tblocks(x)\n",
    "        words_emb = torch.transpose(words_emb,1,2)\n",
    "        sent_emb = x.mean(dim=1) # pool over the time dimension\n",
    "#         print('words_emb:',words_emb.shape,'sent_emb:',sent_emb.shape)\n",
    "\n",
    "        return words_emb,sent_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_models():\n",
    "    # build model ############################################################\n",
    "#     text_encoder = RNN_ENCODER(dataset.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
    "    text_encoder = TEXT_TRANSFORMER_ENCODER(emb=cfg.TEXT.EMBEDDING_DIM\n",
    "                                            ,heads=8\n",
    "                                            ,depth=4\n",
    "                                            ,seq_length=cfg.TEXT.WORDS_NUM\n",
    "                                            ,num_tokens=dataset.n_words)\n",
    "    image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n",
    "    labels = Variable(torch.LongTensor(range(batch_size)))\n",
    "    start_epoch = 0\n",
    "    if cfg.TRAIN.NET_E != '':\n",
    "        print('Loading... ', cfg.TRAIN.NET_E)\n",
    "        state_dict = torch.load(cfg.TRAIN.NET_E)\n",
    "        text_encoder.load_state_dict(state_dict)\n",
    "        print('Load ', cfg.TRAIN.NET_E)\n",
    "        #\n",
    "        name = cfg.TRAIN.NET_E.replace('text_encoder', 'image_encoder')\n",
    "        state_dict = torch.load(name)\n",
    "        image_encoder.load_state_dict(state_dict)\n",
    "        print('Load ', name)\n",
    "\n",
    "        istart = cfg.TRAIN.NET_E.rfind('_') + 8\n",
    "        iend = cfg.TRAIN.NET_E.rfind('.')\n",
    "        start_epoch = cfg.TRAIN.NET_E[istart:iend]\n",
    "        start_epoch = int(start_epoch) + 1\n",
    "        print('start_epoch', start_epoch)\n",
    "    if cfg.CUDA:\n",
    "        text_encoder = text_encoder.cuda()\n",
    "        image_encoder = image_encoder.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "    return text_encoder, image_encoder, labels, start_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using config:\n",
      "{'B_VALIDATION': False,\n",
      " 'CONFIG_NAME': 'DAMSM',\n",
      " 'CUDA': True,\n",
      " 'DATASET_NAME': 'coco',\n",
      " 'DATA_DIR': '../data/coco/',\n",
      " 'GAN': {'B_ATTENTION': True,\n",
      "         'B_DCGAN': False,\n",
      "         'CONDITION_DIM': 100,\n",
      "         'DF_DIM': 64,\n",
      "         'GF_DIM': 128,\n",
      "         'R_NUM': 2,\n",
      "         'Z_DIM': 100},\n",
      " 'GPU_ID': 3,\n",
      " 'RNN_TYPE': 'LSTM',\n",
      " 'TEXT': {'CAPTIONS_PER_IMAGE': 5, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 15},\n",
      " 'TRAIN': {'BATCH_SIZE': 96,\n",
      "           'B_NET_D': True,\n",
      "           'DISCRIMINATOR_LR': 0.0002,\n",
      "           'ENCODER_LR': 0.002,\n",
      "           'FLAG': True,\n",
      "           'GENERATOR_LR': 0.0002,\n",
      "           'MAX_EPOCH': 600,\n",
      "           'NET_E': '',\n",
      "           'NET_G': '',\n",
      "           'RNN_GRAD_CLIP': 0.25,\n",
      "           'SMOOTH': {'GAMMA1': 4.0,\n",
      "                      'GAMMA2': 5.0,\n",
      "                      'GAMMA3': 10.0,\n",
      "                      'LAMBDA': 1.0},\n",
      "           'SNAPSHOT_INTERVAL': 5},\n",
      " 'TREE': {'BASE_SIZE': 299, 'BRANCH_NUM': 1},\n",
      " 'WORKERS': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/MyDataStor1/mmrl/MMRL/notebooks/miscc/config.py:104: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  yaml_cfg = edict(yaml.load(f))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wlosses = []\n",
    "slosses = []\n",
    "\n",
    "args = parse_args()\n",
    "if args.cfg_file is not None:\n",
    "    cfg_from_file(args.cfg_file)\n",
    "\n",
    "if args.gpu_id == -1:\n",
    "    cfg.CUDA = False\n",
    "else:\n",
    "    cfg.GPU_ID = args.gpu_id\n",
    "\n",
    "if args.data_dir != '':\n",
    "    cfg.DATA_DIR = args.data_dir\n",
    "print('Using config:')\n",
    "pprint.pprint(cfg)\n",
    "\n",
    "if not cfg.TRAIN.FLAG:\n",
    "    args.manualSeed = 5000\n",
    "elif args.manualSeed is None:\n",
    "    args.manualSeed = random.randint(1, 10000)\n",
    "random.seed(args.manualSeed)\n",
    "np.random.seed(args.manualSeed)\n",
    "torch.manual_seed(args.manualSeed)\n",
    "if cfg.CUDA:\n",
    "    torch.cuda.manual_seed_all(args.manualSeed)\n",
    "\n",
    "##########################################################################\n",
    "now = datetime.datetime.now(dateutil.tz.tzlocal())\n",
    "timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
    "output_dir = '../output/transfomer_test_{0}_{1}_{2}'.format(cfg.DATASET_NAME, cfg.CONFIG_NAME, timestamp)\n",
    "\n",
    "model_dir = os.path.join(output_dir, 'Model')\n",
    "image_dir = os.path.join(output_dir, 'Image')\n",
    "metrics_dir = os.path.join(output_dir, 'Metrics')\n",
    "mkdir_p(model_dir)\n",
    "mkdir_p(image_dir)\n",
    "mkdir_p(metrics_dir)\n",
    "\n",
    "torch.cuda.set_device(cfg.GPU_ID)\n",
    "cudnn.benchmark = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data loader ##################################################\n",
    "imsize = cfg.TREE.BASE_SIZE * (2 ** (cfg.TREE.BRANCH_NUM-1))\n",
    "batch_size = cfg.TRAIN.BATCH_SIZE\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Scale(int(imsize * 76 / 64)),\n",
    "    transforms.RandomCrop(imsize),\n",
    "    transforms.RandomHorizontalFlip()])\n",
    "dataset = TextDataset(cfg.DATA_DIR, 'train',\n",
    "                      base_size=cfg.TREE.BASE_SIZE,\n",
    "                      transform=image_transform)\n",
    "\n",
    "print(dataset.n_words, dataset.embeddings_num)\n",
    "assert dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, drop_last=True,\n",
    "    shuffle=True, num_workers=int(cfg.WORKERS))\n",
    "\n",
    "# # validation data #\n",
    "dataset_val = TextDataset(cfg.DATA_DIR, 'test',\n",
    "                          base_size=cfg.TREE.BASE_SIZE,\n",
    "                          transform=image_transform)\n",
    "dataloader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=batch_size, drop_last=True,\n",
    "    shuffle=True, num_workers=int(cfg.WORKERS))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ##############################################################\n",
    "text_encoder, image_encoder, labels, start_epoch = build_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = list(text_encoder.parameters())\n",
    "for v in image_encoder.parameters():\n",
    "    if v.requires_grad:\n",
    "        para.append(v)\n",
    "# optimizer = optim.Adam(para, lr=cfg.TRAIN.ENCODER_LR, betas=(0.5, 0.999))\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_dir = '../tensorboard/transfomer_test_{0}_{1}_{2}'.format(cfg.DATASET_NAME, cfg.CONFIG_NAME, timestamp)\n",
    "mkdir_p(tb_dir)\n",
    "tbw = SummaryWriter(log_dir=tb_dir) # Tensorboard logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    lr = cfg.TRAIN.ENCODER_LR\n",
    "    for epoch in range(start_epoch, cfg.TRAIN.MAX_EPOCH):\n",
    "        optimizer = optim.Adam(para, lr=lr, betas=(0.5, 0.999))\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        count = train(dataloader, image_encoder, text_encoder,\n",
    "              batch_size, labels, optimizer, epoch,\n",
    "              dataset.ixtoword, image_dir)\n",
    "        print('-' * 89)\n",
    "        if len(dataloader_val) > 0:\n",
    "            s_loss, w_loss = evaluate(dataloader_val, image_encoder,\n",
    "                                      text_encoder, batch_size)\n",
    "            wlosses.append(w_loss)\n",
    "            slosses.append(s_loss)\n",
    "            print('| end epoch {:3d} | valid loss '\n",
    "                  '{:5.2f} {:5.2f} | lr {:.5f}|'\n",
    "                  .format(epoch, s_loss, w_loss, lr))\n",
    "            tbw.add_scalar('val_w_loss', float(w_loss.item()), epoch)\n",
    "            tbw.add_scalar('val_s_loss', float(s_loss.item()), epoch)\n",
    "        print('-' * 89)\n",
    "        if lr > cfg.TRAIN.ENCODER_LR/10.:\n",
    "            lr *= 0.98\n",
    "\n",
    "        if (epoch % cfg.TRAIN.SNAPSHOT_INTERVAL == 0 or\n",
    "            epoch == cfg.TRAIN.MAX_EPOCH):\n",
    "            torch.save(image_encoder.state_dict(),\n",
    "                       '{0}/image_encoder{1}.pth'.format(model_dir, epoch))\n",
    "            torch.save(text_encoder.state_dict(),\n",
    "                       '{0}/text_encoder{1}.pth'.format(model_dir, epoch))\n",
    "            print('Save G/Ds models.')\n",
    "    df = pd.DataFrame()\n",
    "    df['eval_wlosses']=wlosses\n",
    "    df['eval_slosses']=slosses\n",
    "    df.to_csv('{0}/val_losses.csv'.format(metrics_dir))\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions: tensor([[   10,   115,   485,  ...,    21,     8,   423],\n",
    "#         [   10,    78,   946,  ...,    11,    10,   423],\n",
    "#         [   56,   673,   674,  ...,    47,   795,    11],\n",
    "#         ...,\n",
    "#         [   56,    89,   868,  ...,     0,     0,     0],\n",
    "#         [   10,  1353,   115,  ...,     0,     0,     0],\n",
    "#         [   10, 14329,   115,  ...,     0,     0,     0]], device='cuda:3') torch.Size([96, 15])\n",
    "# b:96, t:15, e:256\n",
    "# positions: torch.Size([96, 15, 256])\n",
    "# x: torch.Size([96, 15, 256])\n",
    "# words_emb: torch.Size([96, 15, 256]) sent_emb: torch.Size([96, 256])\n",
    "# words_emb: torch.Size([96, 15, 256]) , sent_emb: torch.Size([96, 256])\n",
    "# contextT: torch.Size([96, 289, 256]) query: torch.Size([96, 15, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-gpu)",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
